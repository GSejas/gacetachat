#!/usr/bin/env python3
"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸŒŸ Question Answering Engine - LangChain + OpenAI Integration
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“‹ Description:
    AI-powered question answering system using LangChain and OpenAI GPT models.
    Implements document-aware Q&A with FAISS vector similarity search, context
    window management, and source attribution for Costa Rica gazette analysis.

ðŸ—ï¸ Architecture Flow:
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Query     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   User Input    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Query Processor â”‚
    â”‚   (Question)    â”‚              â”‚  (This Module)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â”‚                                 â”‚ Similarity Search
            â–¼                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FAISS Vector DB â”‚              â”‚  Document Chunk  â”‚
    â”‚ (Embeddings)    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Retrieval (k=5) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â”‚ Context Docs                    â”‚ LLM Processing
            â–¼                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Token Limiter  â”‚              â”‚   OpenAI GPT     â”‚
    â”‚  (Context Mgmt) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Answer + Sourcesâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```

ðŸ“¥ Inputs:
    â€¢ User queries: Natural language questions about gazette content
    â€¢ Document indices: FAISS vector stores with embedded text chunks
    â€¢ Model configuration: GPT model selection, temperature, max tokens
    â€¢ Context limits: Token count thresholds for prompt management
    â€¢ Retrieval parameters: Number of similar documents to consider (k=5)

ðŸ“¤ Outputs:
    â€¢ AI answers: Structured responses with source attribution
    â€¢ Source documents: Retrieved text chunks with relevance scores
    â€¢ Token metrics: Context window utilization and optimization stats
    â€¢ Prompt partials: Formatted prompts for debugging and analysis
    â€¢ Error handling: Graceful degradation when information insufficient

ðŸ”— Dependencies:
    â€¢ langchain: Document processing chains and prompt templates
    â€¢ langchain_openai: ChatOpenAI model integration and API client
    â€¢ pydantic: Data validation for structured response models
    â€¢ typing: Type hints for document lists and model interfaces
    â€¢ faiss (via folder_index): Vector similarity search backend

ðŸ›ï¸ Component Relationships:
    ```mermaid
    graph TD
        A[QA Module] --> B[ChatOpenAI]
        A --> C[PromptTemplate]
        A --> D[StuffDocumentsChain]
        A --> E[Document Retrieval]

        F[FAISS Index] --> E
        E --> G[Context Window Mgmt]
        G --> D
        D --> B
        B --> H[Answer + Sources]

        classDef qaModule fill:#e1f5fe
        classDef langchain fill:#f3e5f5
        classDef external fill:#fff3e0

        class A qaModule
        class C,D,G langchain
        class B,F,H external
    ```

ðŸ”’ Security Considerations:
    âš ï¸  HIGH: OpenAI API key exposure via ChatOpenAI initialization
    âš ï¸  MEDIUM: Prompt injection through user queries affecting LLM behavior
    âš ï¸  MEDIUM: Document content leakage via similarity search results
    âš ï¸  LOW: Token count manipulation for cost optimization attacks
    âš ï¸  LOW: Debug mode bypass potentially exposing internal prompts

ðŸ›¡ï¸ Risk Analysis:
    â€¢ API Costs: No rate limiting on expensive GPT-4 calls, budget risk
    â€¢ Data Privacy: User queries and documents sent to OpenAI servers
    â€¢ Response Quality: Hallucination risk when insufficient context available
    â€¢ System Availability: Dependent on OpenAI API uptime and rate limits
    â€¢ Context Injection: Malicious documents could bias LLM responses

âš¡ Performance Characteristics:
    â€¢ Time Complexity: O(d*log(n)) where d=docs, n=vector_dimension
    â€¢ Token Usage: 150-4000 tokens per query depending on context size
    â€¢ Response Time: 2-8 seconds for GPT-4, 1-3 seconds for GPT-3.5
    â€¢ Memory Usage: ~50MB + document index size in memory
    â€¢ Throughput: Limited by OpenAI rate limits (3,500 RPM for GPT-4)

ðŸ§ª Testing Strategy:
    â€¢ Unit Tests: Prompt template validation, document filtering logic
    â€¢ Integration Tests: End-to-end Q&A with known document corpus
    â€¢ Performance Tests: Large document sets, concurrent query processing
    â€¢ Quality Tests: Answer accuracy validation against human benchmarks

ðŸ“Š Monitoring & Observability:
    â€¢ Metrics: Query response times, token usage, source relevance scores
    â€¢ Logging: User queries, retrieved documents, model responses
    â€¢ Alerts: API failures, excessive token usage, low-quality responses
    â€¢ Cost Tracking: OpenAI API usage and billing integration

ðŸ”„ Data Flow:
    ```
    Query Input â”€â”€â–¶ Vector Search â”€â”€â–¶ Context Assembly â”€â”€â–¶ LLM Processing â”€â”€â–¶ Answer Output
         â”‚               â”‚                 â”‚                    â”‚                â”‚
         â–¼               â–¼                 â–¼                    â–¼                â–¼
    Validation    Similarity Scoring   Token Limiting    API Request     Source Attribution
    ```

ðŸ“š Usage Examples:
    ```python
    # Initialize LLM
    llm = get_llm("gpt-4", temperature=0.3, max_tokens=1000)

    # Query with document context
    result = query_folder(
        query="What are the main regulations?",
        folder_index=faiss_index,
        llm=llm,
        return_all=False
    )

    # Access structured response
    answer = result["answer"]
    sources = result["sources"]
    references = result["ai_references"]
    ```

ðŸ”§ Configuration:
    ```python
    # Model Selection
    SUPPORTED_MODELS = ["gpt-4", "gpt-3.5-turbo", "debug"]

    # Context Management
    MAX_CONTEXT_TOKENS = 4000
    SIMILARITY_SEARCH_K = 5

    # Prompt Template
    STUFF_PROMPT_TEMPLATE = "Answer using provided sources..."
    ```

Author: GacetaChat Team | Version: 2.1.0 | Last Updated: 2024-12-19
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

from typing import List

from langchain.chains.combine_documents.stuff import StuffDocumentsChain

# from knowledge_gpt.core.debug import FakeChatModel
from langchain.chat_models.base import BaseChatModel
from langchain.docstore.document import Document
from langchain_openai import ChatOpenAI


def pop_docs_upto_limit(
    query: str, chain: StuffDocumentsChain, docs: List[Document], max_len: int
) -> List[Document]:
    """Pops documents from a list until the final prompt length is less
    than the max length."""

    token_count: int = chain.prompt_length(docs, question=query)  # type: ignore

    while token_count > max_len and len(docs) > 0:
        docs.pop()
        token_count = chain.prompt_length(docs, question=query)  # type: ignore

    return docs


def get_llm(model: str, **kwargs) -> BaseChatModel:
    if model == "debug":
        return None  # FakeChatModel()

    if "gpt" in model:
        return ChatOpenAI(model=model, **kwargs)  # type: ignore

    raise NotImplementedError(f"Model {model} not supported!")


from langchain.prompts import PromptTemplate

## Use a shorter template to reduce the number of tokens in the prompt
template = """Create a final answer to the given questions using the provided document excerpts (given in no particular order) as sources. 

ALWAYS include a "SOURCES" section in your answer citing only the minimal set of sources needed to answer the question. 

If you are unable to answer the question, simply state that you do not have enough information to answer the question and leave the SOURCES section empty. 

Use only the provided documents and do not attempt to fabricate an answer.


---------

QUESTION: {question}
=========
{context}
=========
FINAL ANSWER:"""

STUFF_PROMPT = PromptTemplate(
    template=template, input_variables=["context", "question"]
)
from langchain.chat_models.base import BaseChatModel
from langchain.docstore.document import Document

# from knowledge_gpt.core.embedding import FolderIndex
from pydantic import BaseModel

# <!-- ruff: noqa: F821 -->
# from langchain.globals import set_llm_cache


class AnswerWithSources(BaseModel):
    answer: str
    sources: List[Document]


from langchain.chains.combine_documents import create_stuff_documents_chain

# from langchain.cache import InMemoryCache


# set_llm_cache(InMemoryCache())
def query_folder(
    query: str,
    folder_index,
    llm: BaseChatModel,
    return_all: bool = False,
    debug: bool = True,
):
    """Queries a folder index for an answer.

    Args:
        query (str): The query to search for.
        folder_index (FolderIndex): The folder index to search.
        return_all (bool): Whether to return all the documents from the embedding or
        just the sources for the answer.
        model (str): The model to use for the answer generation.
        **model_kwargs (Any): Keyword arguments for the model.

    Returns:
        AnswerWithSources: The answer and the source documents.
    """
    # if debug:
    #     return {
    #         "answer": query,
    #         "partial": None,
    #         "sources": [],
    #         "ai_references": None,
    #     }

    relevant_docs = folder_index.similarity_search(query, k=5)

    partial_prompt = STUFF_PROMPT.partial(context=relevant_docs, question=query)

    chain = create_stuff_documents_chain(llm, STUFF_PROMPT)

    result = chain.invoke({"context": relevant_docs, "question": query})

    sources = relevant_docs

    answer = result.split("SOURCES:")[0]

    return {
        "answer": answer,
        "partial": partial_prompt,
        "sources": sources,
        "ai_references": result.split("SOURCES:")[1],
    }
